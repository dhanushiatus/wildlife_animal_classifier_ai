{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20b8aaa-c8ae-4fa5-959f-bb5958377728",
   "metadata": {},
   "source": [
    "Wild_Life_Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95d7ee64-7689-4248-8d0c-1864cd7271c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (4.13.0.92)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
      "Requirement already satisfied: tensorflow-hub in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (0.16.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.78.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.13.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-hub) (2.20.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy matplotlib opencv-python pillow tensorflow tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af953b53-8d69-4e8d-a5d9-3bbc8e80fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8473971f-d5a0-4c59-b53d-5ce45d9e2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"animals/animals\"   \n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_ratio = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40bc6f5f-bcaa-42d6-aaec-ac9727ea5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/Test split completed!\n"
     ]
    }
   ],
   "source": [
    "#Split Code\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "dataset_dir = \"animals/animals\"  # Inner folder with class folders\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train/test folders\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each animal class\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    images = [f for f in os.listdir(class_path) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    random.shuffle(images)\n",
    "    split_index = int(len(images) * train_ratio)\n",
    "\n",
    "    train_images = images[:split_index]\n",
    "    test_images = images[split_index:]\n",
    "\n",
    "    # Create class folders in train/test\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    test_class_dir = os.path.join(test_dir, class_name)\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(test_class_dir, exist_ok=True)\n",
    "\n",
    "    # Copy images\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "print(\"âœ… Train/Test split completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2060c0c0-fd81-44a0-9a92-ae1e9b541db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: train | Directories: 90 | Images: 0\n",
      "Path: train\\antelope | Directories: 0 | Images: 60\n",
      "Path: train\\badger | Directories: 0 | Images: 60\n",
      "Path: train\\bat | Directories: 0 | Images: 58\n",
      "Path: train\\bear | Directories: 0 | Images: 58\n",
      "Path: train\\bee | Directories: 0 | Images: 60\n",
      "Path: train\\beetle | Directories: 0 | Images: 59\n",
      "Path: train\\bison | Directories: 0 | Images: 59\n",
      "Path: train\\boar | Directories: 0 | Images: 60\n",
      "Path: train\\butterfly | Directories: 0 | Images: 59\n",
      "Path: train\\cat | Directories: 0 | Images: 60\n",
      "Path: train\\caterpillar | Directories: 0 | Images: 60\n",
      "Path: train\\chimpanzee | Directories: 0 | Images: 59\n",
      "Path: train\\cockroach | Directories: 0 | Images: 60\n",
      "Path: train\\cow | Directories: 0 | Images: 59\n",
      "Path: train\\coyote | Directories: 0 | Images: 60\n",
      "Path: train\\crab | Directories: 0 | Images: 60\n",
      "Path: train\\crow | Directories: 0 | Images: 59\n",
      "Path: train\\deer | Directories: 0 | Images: 60\n",
      "Path: train\\dog | Directories: 0 | Images: 59\n",
      "Path: train\\dolphin | Directories: 0 | Images: 60\n",
      "Path: train\\donkey | Directories: 0 | Images: 59\n",
      "Path: train\\dragonfly | Directories: 0 | Images: 59\n",
      "Path: train\\duck | Directories: 0 | Images: 59\n",
      "Path: train\\eagle | Directories: 0 | Images: 60\n",
      "Path: train\\elephant | Directories: 0 | Images: 60\n",
      "Path: train\\flamingo | Directories: 0 | Images: 60\n",
      "Path: train\\fly | Directories: 0 | Images: 60\n",
      "Path: train\\fox | Directories: 0 | Images: 59\n",
      "Path: train\\goat | Directories: 0 | Images: 60\n",
      "Path: train\\goldfish | Directories: 0 | Images: 60\n",
      "Path: train\\goose | Directories: 0 | Images: 59\n",
      "Path: train\\gorilla | Directories: 0 | Images: 58\n",
      "Path: train\\grasshopper | Directories: 0 | Images: 60\n",
      "Path: train\\hamster | Directories: 0 | Images: 60\n",
      "Path: train\\hare | Directories: 0 | Images: 59\n",
      "Path: train\\hedgehog | Directories: 0 | Images: 60\n",
      "Path: train\\hippopotamus | Directories: 0 | Images: 58\n",
      "Path: train\\hornbill | Directories: 0 | Images: 59\n",
      "Path: train\\horse | Directories: 0 | Images: 60\n",
      "Path: train\\hummingbird | Directories: 0 | Images: 60\n",
      "Path: train\\hyena | Directories: 0 | Images: 59\n",
      "Path: train\\jellyfish | Directories: 0 | Images: 60\n",
      "Path: train\\kangaroo | Directories: 0 | Images: 60\n",
      "Path: train\\koala | Directories: 0 | Images: 60\n",
      "Path: train\\ladybugs | Directories: 0 | Images: 59\n",
      "Path: train\\leopard | Directories: 0 | Images: 60\n",
      "Path: train\\lion | Directories: 0 | Images: 60\n",
      "Path: train\\lizard | Directories: 0 | Images: 60\n",
      "Path: train\\lobster | Directories: 0 | Images: 60\n",
      "Path: train\\mosquito | Directories: 0 | Images: 60\n",
      "Path: train\\moth | Directories: 0 | Images: 58\n",
      "Path: train\\mouse | Directories: 0 | Images: 60\n",
      "Path: train\\octopus | Directories: 0 | Images: 60\n",
      "Path: train\\okapi | Directories: 0 | Images: 59\n",
      "Path: train\\orangutan | Directories: 0 | Images: 59\n",
      "Path: train\\otter | Directories: 0 | Images: 59\n",
      "Path: train\\owl | Directories: 0 | Images: 58\n",
      "Path: train\\ox | Directories: 0 | Images: 60\n",
      "Path: train\\oyster | Directories: 0 | Images: 59\n",
      "Path: train\\panda | Directories: 0 | Images: 60\n",
      "Path: train\\parrot | Directories: 0 | Images: 60\n",
      "Path: train\\pelecaniformes | Directories: 0 | Images: 59\n",
      "Path: train\\penguin | Directories: 0 | Images: 60\n",
      "Path: train\\pig | Directories: 0 | Images: 60\n",
      "Path: train\\pigeon | Directories: 0 | Images: 59\n",
      "Path: train\\porcupine | Directories: 0 | Images: 60\n",
      "Path: train\\possum | Directories: 0 | Images: 60\n",
      "Path: train\\raccoon | Directories: 0 | Images: 60\n",
      "Path: train\\rat | Directories: 0 | Images: 59\n",
      "Path: train\\reindeer | Directories: 0 | Images: 60\n",
      "Path: train\\rhinoceros | Directories: 0 | Images: 60\n",
      "Path: train\\sandpiper | Directories: 0 | Images: 60\n",
      "Path: train\\seahorse | Directories: 0 | Images: 60\n",
      "Path: train\\seal | Directories: 0 | Images: 59\n",
      "Path: train\\shark | Directories: 0 | Images: 60\n",
      "Path: train\\sheep | Directories: 0 | Images: 60\n",
      "Path: train\\snake | Directories: 0 | Images: 60\n",
      "Path: train\\sparrow | Directories: 0 | Images: 60\n",
      "Path: train\\squid | Directories: 0 | Images: 60\n",
      "Path: train\\squirrel | Directories: 0 | Images: 58\n",
      "Path: train\\starfish | Directories: 0 | Images: 60\n",
      "Path: train\\swan | Directories: 0 | Images: 60\n",
      "Path: train\\tiger | Directories: 0 | Images: 60\n",
      "Path: train\\turkey | Directories: 0 | Images: 59\n",
      "Path: train\\turtle | Directories: 0 | Images: 59\n",
      "Path: train\\whale | Directories: 0 | Images: 58\n",
      "Path: train\\wolf | Directories: 0 | Images: 58\n",
      "Path: train\\wombat | Directories: 0 | Images: 60\n",
      "Path: train\\woodpecker | Directories: 0 | Images: 59\n",
      "Path: train\\zebra | Directories: 0 | Images: 60\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(train_dir):\n",
    "    print(f\"Path: {dirpath} | Directories: {len(dirnames)} | Images: {len(filenames)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94ff09a1-7ee8-4a25-89c7-1a5bb5ea5ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5355 images belonging to 90 classes.\n",
      "Found 2625 images belonging to 90 classes.\n",
      "Classes: ['antelope', 'badger', 'bat', 'bear', 'bee', 'beetle', 'bison', 'boar', 'butterfly', 'cat', 'caterpillar', 'chimpanzee', 'cockroach', 'cow', 'coyote', 'crab', 'crow', 'deer', 'dog', 'dolphin', 'donkey', 'dragonfly', 'duck', 'eagle', 'elephant', 'flamingo', 'fly', 'fox', 'goat', 'goldfish', 'goose', 'gorilla', 'grasshopper', 'hamster', 'hare', 'hedgehog', 'hippopotamus', 'hornbill', 'horse', 'hummingbird', 'hyena', 'jellyfish', 'kangaroo', 'koala', 'ladybugs', 'leopard', 'lion', 'lizard', 'lobster', 'mosquito', 'moth', 'mouse', 'octopus', 'okapi', 'orangutan', 'otter', 'owl', 'ox', 'oyster', 'panda', 'parrot', 'pelecaniformes', 'penguin', 'pig', 'pigeon', 'porcupine', 'possum', 'raccoon', 'rat', 'reindeer', 'rhinoceros', 'sandpiper', 'seahorse', 'seal', 'shark', 'sheep', 'snake', 'sparrow', 'squid', 'squirrel', 'starfish', 'swan', 'tiger', 'turkey', 'turtle', 'whale', 'wolf', 'wombat', 'woodpecker', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "#DATA GENERATORS\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "675aba8c-2c9d-4f52-ba2f-0c0453b39f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)                  â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">115,290</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)                  â”‚         \u001b[38;5;34m115,290\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">115,290</span> (450.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m115,290\u001b[0m (450.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">115,290</span> (450.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m115,290\u001b[0m (450.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Wrap hub.KerasLayer in a Lambda layer\n",
    "def build_model(num_classes):\n",
    "    inputs = Input(shape=(224,224,3))\n",
    "    \n",
    "    x = Lambda(lambda x: hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\",\n",
    "        trainable=False\n",
    "    )(x))(inputs)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "eff_model = build_model(len(class_names))\n",
    "eff_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f398bcb0-b4d8-4d94-b758-67f1c59d88eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 0.9643 - loss: 0.2044  \n",
      "Epoch 1: val_accuracy improved from None to 0.97448, saving model to best_model.keras\n",
      "\n",
      "Epoch 1: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.97448, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 1: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 744ms/step - accuracy: 0.9610 - loss: 0.2017 - val_accuracy: 0.9745 - val_loss: 0.1478 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.9733 - loss: 0.1585  \n",
      "Epoch 2: val_accuracy improved from 0.97448 to 0.98590, saving model to best_model.keras\n",
      "\n",
      "Epoch 2: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.97448 to 0.98590, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 2: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 750ms/step - accuracy: 0.9744 - loss: 0.1521 - val_accuracy: 0.9859 - val_loss: 0.1108 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - accuracy: 0.9877 - loss: 0.1026  \n",
      "Epoch 3: val_accuracy improved from 0.98590 to 0.99124, saving model to best_model.keras\n",
      "\n",
      "Epoch 3: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.98590 to 0.99124, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 3: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 729ms/step - accuracy: 0.9845 - loss: 0.1123 - val_accuracy: 0.9912 - val_loss: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505ms/step - accuracy: 0.9921 - loss: 0.0876  \n",
      "Epoch 4: val_accuracy did not improve from 0.99124\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.99124\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 753ms/step - accuracy: 0.9912 - loss: 0.0853 - val_accuracy: 0.9912 - val_loss: 0.0754 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515ms/step - accuracy: 0.9915 - loss: 0.0722  \n",
      "Epoch 5: val_accuracy improved from 0.99124 to 0.99238, saving model to best_model.keras\n",
      "\n",
      "Epoch 5: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.99124 to 0.99238, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 5: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 762ms/step - accuracy: 0.9922 - loss: 0.0722 - val_accuracy: 0.9924 - val_loss: 0.0654 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.9945 - loss: 0.0640  \n",
      "Epoch 6: val_accuracy did not improve from 0.99238\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.99238\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 723ms/step - accuracy: 0.9925 - loss: 0.0638 - val_accuracy: 0.9916 - val_loss: 0.0588 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.9966 - loss: 0.0494  \n",
      "Epoch 7: val_accuracy improved from 0.99238 to 0.99505, saving model to best_model.keras\n",
      "\n",
      "Epoch 7: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.99238 to 0.99505, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 7: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 733ms/step - accuracy: 0.9961 - loss: 0.0513 - val_accuracy: 0.9950 - val_loss: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.9960 - loss: 0.0462  \n",
      "Epoch 8: val_accuracy improved from 0.99505 to 0.99543, saving model to best_model.keras\n",
      "\n",
      "Epoch 8: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.99505 to 0.99543, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 8: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 728ms/step - accuracy: 0.9965 - loss: 0.0456 - val_accuracy: 0.9954 - val_loss: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - accuracy: 0.9983 - loss: 0.0356  \n",
      "Epoch 9: val_accuracy did not improve from 0.99543\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99543\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 751ms/step - accuracy: 0.9983 - loss: 0.0352 - val_accuracy: 0.9939 - val_loss: 0.0433 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 0.9985 - loss: 0.0312  \n",
      "Epoch 10: val_accuracy improved from 0.99543 to 0.99619, saving model to best_model.keras\n",
      "\n",
      "Epoch 10: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.99543 to 0.99619, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 10: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 748ms/step - accuracy: 0.9985 - loss: 0.0320 - val_accuracy: 0.9962 - val_loss: 0.0365 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.9991 - loss: 0.0252  \n",
      "Epoch 11: val_accuracy did not improve from 0.99619\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99619\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 757ms/step - accuracy: 0.9985 - loss: 0.0284 - val_accuracy: 0.9962 - val_loss: 0.0345 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.9987 - loss: 0.0247  \n",
      "Epoch 12: val_accuracy did not improve from 0.99619\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.99619\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 734ms/step - accuracy: 0.9981 - loss: 0.0270 - val_accuracy: 0.9958 - val_loss: 0.0330 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - accuracy: 0.9996 - loss: 0.0217  \n",
      "Epoch 13: val_accuracy improved from 0.99619 to 0.99733, saving model to best_model.keras\n",
      "\n",
      "Epoch 13: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.99619 to 0.99733, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 13: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 720ms/step - accuracy: 0.9993 - loss: 0.0219 - val_accuracy: 0.9973 - val_loss: 0.0292 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.9992 - loss: 0.0195  \n",
      "Epoch 14: val_accuracy improved from 0.99733 to 0.99771, saving model to best_model.keras\n",
      "\n",
      "Epoch 14: finished saving model to best_model.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.99733 to 0.99771, saving model to best_weights.weights.h5\n",
      "\n",
      "Epoch 14: finished saving model to best_weights.weights.h5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 729ms/step - accuracy: 0.9989 - loss: 0.0202 - val_accuracy: 0.9977 - val_loss: 0.0249 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.9991 - loss: 0.0183  \n",
      "Epoch 15: val_accuracy did not improve from 0.99771\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99771\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 716ms/step - accuracy: 0.9994 - loss: 0.0183 - val_accuracy: 0.9970 - val_loss: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - accuracy: 0.9996 - loss: 0.0144  \n",
      "Epoch 16: val_accuracy did not improve from 0.99771\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.99771\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 721ms/step - accuracy: 0.9996 - loss: 0.0155 - val_accuracy: 0.9962 - val_loss: 0.0279 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - accuracy: 0.9998 - loss: 0.0150  \n",
      "Epoch 17: val_accuracy did not improve from 0.99771\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.99771\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 722ms/step - accuracy: 0.9998 - loss: 0.0144 - val_accuracy: 0.9970 - val_loss: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 0.9997 - loss: 0.0127  \n",
      "Epoch 18: val_accuracy did not improve from 0.99771\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.99771\n",
      "\u001b[1m168/168\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 735ms/step - accuracy: 0.9998 - loss: 0.0123 - val_accuracy: 0.9966 - val_loss: 0.0254 - learning_rate: 2.0000e-04\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "âœ… Final model saved as wild_model_final.keras\n",
      "âœ… Final weights saved as wild_model_final.weights.h5\n",
      "\n",
      "ğŸ“Š Model file size: 1.34 MB\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "eff_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add more callbacks for better training\n",
    "callbacks = [\n",
    "    # Save the full model, not just weights\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\",  # Save full model\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Save weights as backup\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_weights.weights.h5\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Stop if no improvement\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate if plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model with MORE EPOCHS\n",
    "history = eff_model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=50,  # Increase to 50 epochs\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# After training, save the final model\n",
    "eff_model.save(\"wild_model_final.keras\")\n",
    "print(\"âœ… Final model saved as wild_model_final.keras\")\n",
    "\n",
    "# Also save weights separately\n",
    "eff_model.save_weights(\"wild_model_final.weights.h5\")\n",
    "print(\"âœ… Final weights saved as wild_model_final.weights.h5\")\n",
    "\n",
    "# Check file sizes\n",
    "import os\n",
    "print(f\"\\nğŸ“Š Model file size: {os.path.getsize('wild_model_final.keras') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5adaacba-9b02-4ee0-9539-9eebd838c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ FIXING MODEL SAVING IN JUPYTER NOTEBOOK\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Available files:\n",
      "   - best_model.keras: 1.34 MB\n",
      "   - best_weights.weights.h5: 1.34 MB\n",
      "   - check_model.py: 0.00 MB\n",
      "   - efficient_net_model.weights.h5: 1.34 MB\n",
      "   - wild_model.keras: 1.34 MB\n",
      "   - wild_model.weights.h5: 1.34 MB\n",
      "   - wild_model_architecture.json: 0.00 MB\n",
      "   - wild_model_complete.keras: 1.34 MB\n",
      "   - wild_model_final.keras: 1.34 MB\n",
      "   - wild_model_final.weights.h5: 1.34 MB\n",
      "\n",
      "ğŸ”„ Step 1: Attempting to load best_model.keras...\n",
      "âŒ Could not load best_model.keras: Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  â€¢ args=('<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=input_layer_2>',)\n",
      "  â€¢ kwargs={'mask': 'None'}\n",
      "\n",
      "ğŸ”„ Step 2: Attempting to rebuild model from weights...\n",
      "âœ… Found weights file: best_weights.weights.h5\n",
      "ğŸ—ï¸ Rebuilding model architecture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x00000202096E02D0> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m  \u001b[38;5;66;03m# Your number of classes\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Build with Sequential (more compatible)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     61\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInputLayer(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     62\u001b[0m     hub\u001b[38;5;241m.\u001b[39mKerasLayer(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/efficientnet/b0/feature-vector/1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     ),\n\u001b[0;32m     66\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m ])\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Try to load weights\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\models\\sequential.py:75\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, trainable, name)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layers:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(layer, rebuild\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_rebuild()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\models\\sequential.py:97\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m     95\u001b[0m         layer \u001b[38;5;241m=\u001b[39m origin_layer\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer):\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly instances of `keras.Layer` can be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded to a Sequential model. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_layer_name_unique(layer):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll layers added to a Sequential model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have unique names. Name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe name of a layer in this model. Update the `name` argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto pass a unique name.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x00000202096E02D0> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "\n",
    "print(\"ğŸ”§ FIXING MODEL SAVING IN JUPYTER NOTEBOOK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's check what files we have\n",
    "print(\"\\nğŸ“ Available files:\")\n",
    "for file in os.listdir():\n",
    "    if 'model' in file or 'weight' in file or 'best' in file:\n",
    "        size = os.path.getsize(file) / (1024 * 1024) if os.path.isfile(file) else 0\n",
    "        print(f\"   - {file}: {size:.2f} MB\")\n",
    "\n",
    "# Step 1: Try to load the best model with safe_mode=False\n",
    "print(\"\\nğŸ”„ Step 1: Attempting to load best_model.keras...\")\n",
    "try:\n",
    "    # Disable safe mode to allow lambda loading\n",
    "    tf.keras.config.enable_unsafe_deserialization()\n",
    "    \n",
    "    best_model = tf.keras.models.load_model(\n",
    "        \"best_model.keras\",\n",
    "        custom_objects={'KerasLayer': hub.KerasLayer},\n",
    "        safe_mode=False\n",
    "    )\n",
    "    print(\"âœ… Successfully loaded best_model.keras!\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_input = np.random.random((1, 224, 224, 3))\n",
    "    predictions = best_model.predict(test_input, verbose=0)[0]\n",
    "    print(f\"ğŸ“Š Model test confidence: {predictions.max():.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not load best_model.keras: {e}\")\n",
    "    best_model = None\n",
    "\n",
    "# Step 2: If that didn't work, try to rebuild from weights\n",
    "if best_model is None:\n",
    "    print(\"\\nğŸ”„ Step 2: Attempting to rebuild model from weights...\")\n",
    "    \n",
    "    # Find weights file\n",
    "    weights_file = None\n",
    "    for file in ['best_weights.weights.h5', 'wild_model_final.weights.h5', \n",
    "                 'efficient_net_model.weights.h5']:\n",
    "        if os.path.exists(file):\n",
    "            weights_file = file\n",
    "            print(f\"âœ… Found weights file: {file}\")\n",
    "            break\n",
    "    \n",
    "    if weights_file:\n",
    "        # Rebuild model architecture\n",
    "        print(\"ğŸ—ï¸ Rebuilding model architecture...\")\n",
    "        \n",
    "        num_classes = 90  # Your number of classes\n",
    "        \n",
    "        # Build with Sequential (more compatible)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "            hub.KerasLayer(\n",
    "                \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\",\n",
    "                trainable=False\n",
    "            ),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Try to load weights\n",
    "        try:\n",
    "            model.load_weights(weights_file)\n",
    "            print(\"âœ… Weights loaded successfully!\")\n",
    "            best_model = model\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Could not load weights: {e}\")\n",
    "            best_model = None\n",
    "    else:\n",
    "        print(\"âŒ No weights file found!\")\n",
    "        best_model = None\n",
    "\n",
    "# Step 3: If we have a model, test and save it properly\n",
    "if best_model is not None:\n",
    "    print(\"\\nâœ… Step 3: Model loaded successfully!\")\n",
    "    \n",
    "    # Compile the model\n",
    "    best_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Test thoroughly\n",
    "    print(\"\\nğŸ” Testing model...\")\n",
    "    test_input = np.random.random((1, 224, 224, 3))\n",
    "    predictions = best_model.predict(test_input, verbose=0)[0]\n",
    "    \n",
    "    print(f\"   Prediction shape: {predictions.shape}\")\n",
    "    print(f\"   Min confidence: {predictions.min():.4f}\")\n",
    "    print(f\"   Max confidence: {predictions.max():.4f}\")\n",
    "    print(f\"   Mean confidence: {predictions.mean():.4f}\")\n",
    "    \n",
    "    if predictions.max() > 0.5:\n",
    "        print(\"âœ… MODEL HAS TRAINED WEIGHTS! (Good)\")\n",
    "    elif predictions.max() > 0.1:\n",
    "        print(\"âš ï¸ Model has some training but confidence is low\")\n",
    "    else:\n",
    "        print(\"âŒ Model has flat predictions - weights not loaded properly\")\n",
    "    \n",
    "    # Save in multiple formats\n",
    "    print(\"\\nğŸ’¾ Step 4: Saving model in multiple formats...\")\n",
    "    \n",
    "    # Format 1: SavedModel (most compatible)\n",
    "    best_model.save(\"final_model_saved\", save_format='tf')\n",
    "    print(\"âœ… Saved as 'final_model_saved' folder\")\n",
    "    \n",
    "    # Check folder size\n",
    "    folder_size = 0\n",
    "    for path, dirs, files in os.walk(\"final_model_saved\"):\n",
    "        for f in files:\n",
    "            fp = os.path.join(path, f)\n",
    "            folder_size += os.path.getsize(fp)\n",
    "    folder_size_mb = folder_size / (1024 * 1024)\n",
    "    print(f\"ğŸ“Š SavedModel folder size: {folder_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Create zip\n",
    "    shutil.make_archive(\"final_model\", 'zip', \"final_model_saved\")\n",
    "    zip_size = os.path.getsize(\"final_model.zip\") / (1024 * 1024)\n",
    "    print(f\"âœ… Created 'final_model.zip' ({zip_size:.2f} MB)\")\n",
    "    \n",
    "    # Format 2: Keras format (with safe_mode=False metadata)\n",
    "    best_model.save(\"final_model.keras\")\n",
    "    keras_size = os.path.getsize(\"final_model.keras\") / (1024 * 1024)\n",
    "    print(f\"âœ… Saved as 'final_model.keras' ({keras_size:.2f} MB)\")\n",
    "    \n",
    "    # Format 3: H5 format\n",
    "    best_model.save(\"final_model.h5\")\n",
    "    h5_size = os.path.getsize(\"final_model.h5\") / (1024 * 1024)\n",
    "    print(f\"âœ… Saved as 'final_model.h5' ({h5_size:.2f} MB)\")\n",
    "    \n",
    "    # Format 4: Weights only\n",
    "    best_model.save_weights(\"final_weights.weights.h5\")\n",
    "    weights_size = os.path.getsize(\"final_weights.weights.h5\") / (1024 * 1024)\n",
    "    print(f\"âœ… Saved weights as 'final_weights.weights.h5' ({weights_size:.2f} MB)\")\n",
    "    \n",
    "    # Final verification\n",
    "    print(\"\\nğŸ” Step 5: Final verification...\")\n",
    "    print(\"Testing that saved models load correctly:\")\n",
    "    \n",
    "    # Test SavedModel\n",
    "    test_model = tf.keras.models.load_model(\n",
    "        \"final_model_saved\",\n",
    "        custom_objects={'KerasLayer': hub.KerasLayer}\n",
    "    )\n",
    "    test_pred = test_model.predict(test_input, verbose=0)[0]\n",
    "    print(f\"   âœ… SavedModel test: {test_pred.max():.2%}\")\n",
    "    \n",
    "    # Test zip (extract and load)\n",
    "    with h5py.File(\"final_model.keras\", 'r') as f:\n",
    "        if 'model_weights' in f:\n",
    "            print(\"   âœ… Keras file contains weights\")\n",
    "    \n",
    "    print(\"\\nâœ… ALL DONE! Use these files in your Streamlit app:\")\n",
    "    print(\"   - final_model_saved/ (folder) - Best for Streamlit\")\n",
    "    print(\"   - final_model.zip - Easy to transfer\")\n",
    "    print(\"   - final_model.keras - Alternative format\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ Could not load model. Please check your training files.\")\n",
    "    \n",
    "    # List all files for debugging\n",
    "    print(\"\\nğŸ“ All files in directory:\")\n",
    "    for file in os.listdir():\n",
    "        if os.path.isfile(file):\n",
    "            size = os.path.getsize(file) / 1024\n",
    "            print(f\"   {file}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83f75305-3964-416d-8f4c-da0b4dcb3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking model weights...\n",
      "ğŸ“Š Prediction stats:\n",
      "   Min: 0.0006\n",
      "   Max: 0.0941\n",
      "   Mean: 0.0111\n",
      "âŒ Model predictions are flat - model may not be trained yet!\n"
     ]
    }
   ],
   "source": [
    "# Check if your model actually has trained weights\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ” Checking model weights...\")\n",
    "\n",
    "# Make a test prediction with random data\n",
    "test_input = np.random.random((1, 224, 224, 3))\n",
    "predictions = eff_model.predict(test_input, verbose=0)\n",
    "\n",
    "print(f\"ğŸ“Š Prediction stats:\")\n",
    "print(f\"   Min: {predictions.min():.4f}\")\n",
    "print(f\"   Max: {predictions.max():.4f}\")\n",
    "print(f\"   Mean: {predictions.mean():.4f}\")\n",
    "\n",
    "if predictions.max() > 0.1:\n",
    "    print(\"âœ… Model has trained weights!\")\n",
    "else:\n",
    "    print(\"âŒ Model predictions are flat - model may not be trained yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df96377-7d92-4f7d-a920-feccfaf969b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
